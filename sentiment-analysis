{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20122,"status":"ok","timestamp":1681674312873,"user":{"displayName":"Hisyam Haryo","userId":"17266018885690485961"},"user_tz":-420},"id":"BUx-2Ki6RQ0w","outputId":"7e5d58b4-6c0a-4424-fc07-82700a4eb97a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","\n","drive.mount(\"/content/drive\")\n"]},{"cell_type":"code","source":["!pip install Singkatan"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lCOVwMDVq11_","executionInfo":{"status":"ok","timestamp":1681674325646,"user_tz":-420,"elapsed":8001,"user":{"displayName":"Hisyam Haryo","userId":"17266018885690485961"}},"outputId":"0d68fee2-9a13-42ac-b42a-ae57283e7a18"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting Singkatan\n","  Downloading Singkatan-0.1.5.tar.gz (9.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from Singkatan) (1.22.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from Singkatan) (1.5.3)\n","Requirement already satisfied: pathlib in /usr/local/lib/python3.9/dist-packages (from Singkatan) (1.0.1)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->Singkatan) (2022.7.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->Singkatan) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->Singkatan) (1.16.0)\n","Building wheels for collected packages: Singkatan\n","  Building wheel for Singkatan (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for Singkatan: filename=Singkatan-0.1.5-py3-none-any.whl size=8814 sha256=0326254f5f5d8d295e26ae2eeeb3fc71e4e5c970a8f235fd888e1df144fcc8e6\n","  Stored in directory: /root/.cache/pip/wheels/5e/00/54/b5ae97a8b8adce0acfdf42f79cfa1fc9fb3fe9b5d5a91119fe\n","Successfully built Singkatan\n","Installing collected packages: Singkatan\n","Successfully installed Singkatan-0.1.5\n"]}]},{"cell_type":"code","source":["import csv\n","from Singkatan.SingkatanConverter import SingkatanConverter\n","from google.colab import files\n","import string"],"metadata":{"id":"8Jm_SWKY578z","executionInfo":{"status":"ok","timestamp":1681678623323,"user_tz":-420,"elapsed":896,"user":{"displayName":"Hisyam Haryo","userId":"17266018885690485961"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["# Algoritma Perbandingan"],"metadata":{"id":"kE2SSEoXipRW"}},{"cell_type":"code","source":["class PerbandinganSentimentAnalyzer:\n","    def __init__(self, sentiment_file, singkatan_file):\n","        self.sentiment_dict = {}\n","        self.singkatan_converter = SingkatanConverter()\n","        # singkatan_file = singkatan_file.translate(str.maketrans('', '', string.punctuation))\n","        self.singkatan_converter.importDictionary(singkatan_file)\n","        \n","        # Load sentiment dictionary from file\n","        with open(sentiment_file, 'r') as f:\n","            reader = csv.reader(f, delimiter=',')\n","            for row in reader:\n","                self.sentiment_dict[row[0]] = int(row[1])\n","\n","    def convert_singkatan(self, sentence):\n","      \n","        converter = SingkatanConverter()\n","        \n","        converter.importDictionary(singkatan_file)\n","        tokens = sentence.split() \n","        converted = [converter.convert(token) for token in tokens]\n","        return ' '.join(converted)\n","\n","    def analyze_sentiment(self, sentence):\n","        # sentence = convert_singkatan(sentence)\n","        max_sentiment = 0\n","        min_sentiment = 0\n","        final_score = 0\n","        result = \"\"\n","        sentiment_detail = []\n","        prev_word = None\n","        words = sentence.lower().split()\n","        for i in range(len(words)):\n","            word = words[i]\n","            if word in self.sentiment_dict:\n","                score = self.sentiment_dict[word]\n","                if prev_word in [\"belum\", \"bukan\", \"tak\", \"tanpa\", \"tidak\", \"pantang\", \"jangan\", \"bukanlah\", \"sok\", \"tidak pernah\"]:\n","                    score = -1 * score  # Invert sentiment score after negation word\n","                if score < min_sentiment:\n","                   min_sentiment = score\n","                if score > max_sentiment:\n","                    max_sentiment = score\n","                sentiment_detail.append({\"kata\": word, \"nilai\": score, \"sentimen\": \"positive\" if score > 0 else \"negative\"})\n","\n","            else:\n","                sentiment_detail.append({\"kata\": word, \"nilai\": 0, \"sentimen\": \"neutral\"})\n","            prev_word = word\n","        \n","        if final_score == 0:\n","            result = \"neutral\"\n","        elif final_score > 0:\n","            result =\"positive\"\n","        else: \n","            result =\"negative\"\n","        \n","        if abs(min_sentiment) > abs(max_sentiment):\n","          final_score = min_sentiment\n","          result = \"negative\"\n","          return {\"sentiment\": result,  \"max_sentiment\":max_sentiment, \"min_sentiment\":min_sentiment, \"final_score\":final_score, \"detail\": sentiment_detail}\n","        elif abs(max_sentiment) > abs(min_sentiment):\n","          final_score = max_sentiment\n","          result = \"positive\"\n","          return {\"sentiment\": result, \"score\": 0, \"max_sentiment\":max_sentiment, \"min_sentiment\":min_sentiment, \"final_score\":final_score, \"detail\": sentiment_detail}\n","        elif abs(min_sentiment) == abs(max_sentiment):\n","          final_score = 0\n","          result = \"neutral\"\n","          return {\"sentiment\": result, \"score\": 0, \"max_sentiment\":max_sentiment, \"min_sentiment\":min_sentiment, \"final_score\":final_score, \"detail\": sentiment_detail}\n","        \n","\n","# Load sentiment dictionary from CSV file\n","sentiment_file = \"/content/drive/My Drive/dataset/sentiment_lexicon/sentiment_corpus/SentimentCorpus.csv\"\n","singkatan_file = \"/content/drive/My Drive/dataset/singkatan/singkatan.csv\"\n","\n","perbandingan_analyzer = PerbandinganSentimentAnalyzer(sentiment_file, singkatan_file)\n","\n","sentence = \"KORUPsi anarkis ganteng adil üòÅ\"\n","sentence = sentence.lower()\n","sentence = perbandingan_analyzer.convert_singkatan(sentence)\n","result = perbandingan_analyzer.analyze_sentiment(sentence)\n","\n","# Preprocessing sentence\n","# sentence = convert_singkatan(sentence)\n","print(\"Kalimat: {}\".format(sentence))\n","print(\"Sentimen: {}\".format(result[\"sentiment\"]))\n","print(\"Max Sentiment: {}\".format(result[\"max_sentiment\"]))\n","print(\"Min Sentiment: {}\".format(result[\"min_sentiment\"]))\n","print(\"Nilai sentimen: {}\".format(result[\"final_score\"]))\n","print(\"Detail sentimen:\")\n","for detail in result[\"detail\"]:\n","    print(\"- kata: {}, nilai: {}, sentimen: {}\".format(detail[\"kata\"], detail[\"nilai\"], detail[\"sentimen\"]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":131},"id":"6lfIUwYZI8_u","executionInfo":{"status":"error","timestamp":1681679373150,"user_tz":-420,"elapsed":390,"user":{"displayName":"Hisyam Haryo","userId":"17266018885690485961"}},"outputId":"0eea2300-73ca-4a68-ca3f-fca1fa4203a2"},"execution_count":24,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-76f00753583c>\"\u001b[0;36m, line \u001b[0;32m75\u001b[0m\n\u001b[0;31m    sentence = \"KORUPsi anarkis ganteng adil \"üòÅ\"\"\u001b[0m\n\u001b[0m                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character 'üòÅ' (U+1F601)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","df1 = pd.read_csv('/content/drive/My Drive/dataset/singkatan/singkatan.csv', delimiter=';')\n","print(df1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UvNwt-e1mvg4","executionInfo":{"status":"ok","timestamp":1681674527761,"user_tz":-420,"elapsed":2,"user":{"displayName":"Hisyam Haryo","userId":"17266018885690485961"}},"outputId":"3e6ecc2e-3d65-4a1e-8ca4-d6a3c71da180"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["       aamiin     amin \n","0        adek     adik \n","1        adlh   adalah \n","2         aer      air \n","3     aiskrim  es krim \n","4          aj     saja \n","...       ...       ...\n","1305      yup       ya \n","1306     yups        ya\n","1307     yupz        ya\n","1308     ywdh  ya sudah\n","1309       ??   bahagia\n","\n","[1310 rows x 2 columns]\n"]}]},{"cell_type":"code","source":["# input_file = \"/content/drive/My Drive/dataset/fake_news/testing/merged-noToken.csv\"\n","input_file = \"/content/drive/My Drive/dataset/fake_news/DataTesting.csv\"\n","with open(input_file, 'r') as f:\n","    reader = csv.reader(f, delimiter=',')\n","    rows = []\n","    for row in reader:\n","        rows.append(row)\n","    for row in reader:\n","        # tanggal = row[0]\n","        # sentence = row[1]\n","        # url = row[2]\n","        judul = row[0]\n","        url = row[1]\n","        label = [2]\n","        result = perbandingan_analyzer.analyze_sentiment(judul)\n","        # print(\"Kalimat: {}\".format(sentence))\n","        # print(\"Sentimen: {}\".format(result[\"sentiment\"]))\n","        # print(\"Nilai sentimen: {}\".format(result[\"final_score\"]))\n","        # print(\"Detail sentimen:\")\n","        # for detail in result[\"detail\"]:\n","        #     print(\"- kata: {}, nilai: {}, sentimen: {}\".format(detail[\"kata\"], detail[\"nilai\"], detail[\"sentimen\"]))\n","            \n","with open('Perbandingan.csv', 'a', newline='') as f:\n","  \n","   writer = csv.writer(f)\n","   writer.writerow([ 'tanggal', 'judul', 'url','label','sentimen'])\n","  #  writer.writerow([ 'judul', 'url','label' ,'sentimen'])\n","   \n","   for row in rows[1:]:\n","      tanggal = row[0]\n","      sentence = row[1]\n","      url = row[2]\n","      label = row[3]\n","      # judul = row[0]\n","      # url = row[1]\n","      # label = row[2]\n","      result = perbandingan_analyzer.analyze_sentiment(sentence)\n","      writer.writerow([tanggal, sentence, url,label, result[\"final_score\"]])\n","      # writer.writerow([judul, url, label, result[\"final_score\"]])\n","      \n","files.download('Perbandingan.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"fhWjXxGomAf0","executionInfo":{"status":"ok","timestamp":1681468076926,"user_tz":-420,"elapsed":373,"user":{"displayName":"Hisyam Haryo","userId":"17266018885690485961"}},"outputId":"73065d40-0957-4d19-f83a-410c6070750b"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_646a208d-da7a-422d-8c33-17610018c05c\", \"Perbandingan.csv\", 184959)"]},"metadata":{}}]},{"cell_type":"markdown","source":["# Algoritma Penjumlahan"],"metadata":{"id":"IN7Q_AVziy9B"}},{"cell_type":"code","source":["import csv\n","from google.colab import files\n","from Singkatan.SingkatanConverter import SingkatanConverter\n","\n","class PenjumlahanSentimentAnalyzer:\n","    def __init__(self, sentiment_file, singkatan_file):\n","        self.sentiment_dict = {}\n","        self.singkatan_converter = SingkatanConverter()\n","        self.singkatan_converter.importDictionary(singkatan_file)\n","        \n","        # Load sentiment dictionary from file\n","        with open(sentiment_file, 'r') as f:\n","            reader = csv.reader(f, delimiter=',')\n","            for row in reader:\n","                self.sentiment_dict[row[0]] = int(row[1])\n","\n","    def convert_singkatan(self, sentence):\n","        converter = SingkatanConverter()\n","        converter.importDictionary(singkatan_file)\n","        tokens = sentence.split() \n","        converted = [converter.convert(token) for token in tokens]\n","        return ' '.join(converted)\n","    \n","    def analyze_sentiment(self, sentence):\n","        sentiment_score = 0\n","        max_sentiment = 0\n","        result = \"\"\n","        sentiment_detail = []\n","        prev_word = None\n","        words = sentence.lower().split()\n","        for i in range(len(words)):\n","            word = words[i]\n","            if word in self.sentiment_dict:\n","                score = self.sentiment_dict[word]\n","                if prev_word in [\"belum\", \"bukan\", \"tak\", \"tanpa\", \"tidak\", \"pantang\", \"jangan\", \"bukanlah\", \"sok\", \"tidak pernah\"]:\n","                    score = -1 * score  # Invert sentiment score after negation word\n","                sentiment_score += score\n","                # if abs(score) > max_sentiment:\n","                #     max_sentiment == abs(score)\n","                sentiment_detail.append({\"kata\": word, \"nilai\": score, \"sentimen\": \"positive\" if score > 0 else \"negative\"})\n","\n","            else:\n","                sentiment_detail.append({\"kata\": word, \"nilai\": 0, \"sentimen\": \"neutral\"})\n","            prev_word = word\n","        \n","        if sentiment_score == 0:\n","            result = \"neutral\"\n","        elif sentiment_score > 0:\n","            result =\"positive\"\n","        else: \n","            result =\"negative\"\n","        return {\"sentiment\": result, \"score\": sentiment_score, \"detail\": sentiment_detail}\n","\n","# Load sentiment dictionary from CSV file\n","sentiment_file = \"/content/drive/My Drive/dataset/sentiment_lexicon/sentiment_corpus/SentimentCorpus.csv\"\n","singkatan_file = \"/content/drive/My Drive/dataset/singkatan/singkatan.csv\"\n","penjumlahan_analyzer = PenjumlahanSentimentAnalyzer(sentiment_file,singkatan_file)\n","\n","sentence = \" anarkis ganteng adil adek\"\n","sentence = perbandingan_analyzer.convert_singkatan(sentence)\n","result = penjumlahan_analyzer.analyze_sentiment(sentence)\n","print(\"Kalimat: {}\".format(sentence))\n","print(\"Sentimen: {}\".format(result[\"sentiment\"]))\n","print(\"Nilai sentimen: {}\".format(result[\"score\"]))\n","print(\"Detail sentimen:\")\n","for detail in result[\"detail\"]:\n","    print(\"- kata: {}, nilai: {}, sentimen: {}\".format(detail[\"kata\"], detail[\"nilai\"], detail[\"sentimen\"]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OHqtTDdIo40O","executionInfo":{"status":"ok","timestamp":1681662717348,"user_tz":-420,"elapsed":3,"user":{"displayName":"Hisyam Haryo","userId":"17266018885690485961"}},"outputId":"e57dad3f-6f4b-43df-bc24-032db5e30809"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Kalimat: anarkis ganteng adil adik\n","Sentimen: positive\n","Nilai sentimen: 4\n","Detail sentimen:\n","- kata: anarkis, nilai: -5, sentimen: negative\n","- kata: ganteng, nilai: 4, sentimen: positive\n","- kata: adil, nilai: 5, sentimen: positive\n","- kata: adik, nilai: 0, sentimen: neutral\n"]}]},{"cell_type":"code","source":["# input_file = \"/content/drive/My Drive/dataset/fake_news/testing/merged-noToken.csv\"\n","input_file = \"/content/drive/My Drive/dataset/fake_news/DataTraining1-comma.csv\"\n","with open(input_file, 'r') as f:\n","    reader = csv.reader(f, delimiter=';')\n","    rows = []\n","    for row in reader:\n","        rows.append(row)\n","    for row in reader:\n","        # tanggal = row[0]\n","        # sentence = row[1]\n","        # url = row[2]\n","        judul = row[0]\n","        url = row[1]\n","        label = [2]\n","        result = penjumlahan_analyzer.analyze_sentiment(judul)\n","        # print(\"Kalimat: {}\".format(sentence))\n","        # print(\"Sentimen: {}\".format(result[\"sentiment\"]))\n","        # print(\"Nilai sentimen: {}\".format(result[\"score\"]))\n","        # print(\"Detail sentimen:\")\n","        # for detail in result[\"detail\"]:\n","        #     print(\"- kata: {}, nilai: {}, sentimen: {}\".format(detail[\"kata\"], detail[\"nilai\"], detail[\"sentimen\"]))\n","            \n","with open('Penjumlahan.csv', 'a', newline='') as f:\n","  \n","   writer = csv.writer(f)\n","   writer.writerow([ 'tanggal', 'judul', 'url','sentimen'])\n","  #  writer.writerow([ 'judul', 'url','label' ,'sentimen'])\n","   \n","   for row in rows[1:]:\n","      tanggal = row[0]\n","      sentence = row[1]\n","      url = row[2]\n","      # judul = row[0]\n","      # url = row[1]\n","      # label = row[2]\n","      result = penjumlahan_analyzer.analyze_sentiment(sentence)\n","      writer.writerow([tanggal, sentence, url, result[\"score\"]])\n","      # writer.writerow([judul, url, label, result[\"score\"]])\n","      \n","files.download('Penjumlahan.csv')"],"metadata":{"id":"U_SsmOEc9AfB","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1680108245073,"user_tz":-420,"elapsed":436,"user":{"displayName":"Hisyam Haryo","userId":"17266018885690485961"}},"outputId":"1c7e7262-de0c-4aea-fd95-ff4afe9faa78"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_b01cf509-8647-4248-ae3a-649c720a504c\", \"Penjumlahan.csv\", 1725473)"]},"metadata":{}}]},{"cell_type":"markdown","source":["# REFERENSI"],"metadata":{"id":"QIm64m2zUaFs"}},{"cell_type":"code","source":["import re\n","from collections import OrderedDict\n","import numpy as np\n","\n","class sentistrength:\n","    def __init__(self, config=dict()):\n","        self.negasi = [line.replace('\\n','') for line in open(\"/content/drive/My Drive/sentiment_analysis/sentistrength_id/negatingword.txt\").read().splitlines()]\n","        self.tanya = [line.replace('\\n','') for line in open(\"/content/drive/My Drive/sentiment_analysis/sentistrength_id/questionword.txt\").read().splitlines()]\n","        #create sentiment words dictionary\n","        self.sentiwords_txt = [line.replace('\\n','').split(\":\") for line in open(\"/content/drive/My Drive/sentiment_analysis/sentistrength_id/sentiwords_id.txt\").read().splitlines()]\n","        self.sentiwords_dict = OrderedDict()\n","        for term in self.sentiwords_txt:\n","            self.sentiwords_dict[term[0]] = int(term[1])\n","        #create emoticon dictionary\n","        self.emoticon_txt = [line.replace('\\n','').split(\" | \") for line in open(\"/content/drive/My Drive/sentiment_analysis/sentistrength_id/emoticon_id.txt\").read().splitlines()]\n","        self.emoticon_dict = OrderedDict()\n","        for term in self.emoticon_txt:\n","            self.emoticon_dict[term[0]] = int(term[1])\n","        #create idioms dictionary\n","        self.idioms_txt = [line.replace('\\n','').split(\":\") for line in open(\"/content/drive/My Drive/sentiment_analysis/sentistrength_id/idioms_id.txt\").read().splitlines()]\n","        self.idioms_dict = OrderedDict()\n","        for term in self.idioms_txt:\n","            self.idioms_dict[term[0]] = int(term[1])\n","        #create boosterwords dictionary\n","        self.boosterwords_txt = [line.replace('\\n','').split(\":\") for line in open(\"/content/drive/My Drive/sentiment_analysis/sentistrength_id/boosterwords_id.txt\").read().splitlines()]\n","        self.boosterwords_dict = OrderedDict()\n","        for term in self.boosterwords_txt:\n","            self.boosterwords_dict[term[0]] = int(term[1])\n","        self.negation_conf = config[\"negation\"]\n","        self.booster_conf = config[\"booster\"]\n","        self.ungkapan_conf = config[\"ungkapan\"]\n","        self.consecutive_conf = config[\"consecutive\"]\n","        self.repeated_conf = config[\"repeated\"]\n","        self.emoticon_conf = config[\"emoticon\"]\n","        self.question_conf = config[\"question\"]\n","        self.exclamation_conf = config[\"exclamation\"]\n","        self.punctuation_conf = config[\"punctuation\"]\n","        self.mean_conf = False\n","\n","    def senti(self,term):\n","        try:\n","            return self.sentiwords_dict[term]\n","        except:\n","            return 0\n","\n","    def emosikon(self,term):\n","        try:\n","            return self.emoticon_dict[term]\n","        except:\n","            return 0\n","\n","    def ungkapan(self,term):\n","        try:\n","            return self.idioms_dict[term]\n","        except:\n","            return 0\n","\n","    def booster(self, term):\n","        try:\n","            return self.boosterwords_dict[term]\n","        except:\n","            return 0\n","\n","    def cek_negationword(self, prev_term, prev_term2):\n","        #jika kata sebelumnya (index-1) adalah kata negasi, negasikan nilai -+nya\n","        if prev_term in self.negasi or prev_term2+\" \"+prev_term in self.negasi:\n","            # print prev_term\n","            self.score = -abs(self.score) if self.score>0 else abs(self.score)\n","\n","    def cek_boosterword(self,term):\n","        booster_score = self.booster(term)\n","        if booster_score !=0 and self.score>0: self.score += booster_score\n","        if booster_score !=0 and self.score<0: self.score -= booster_score\n","\n","    def cek_consecutive_term(self, prev_term):\n","        if self.prev_score>0 and self.score >=3: self.score+=1 \n","        if self.prev_score<0 and self.score <=-3: self.score-=1 \n","\n","    def cek_ungkapan(self, bigram,trigram, i):\n","        bigram = ' '.join(bigram)\n","        trigram = ' '.join(trigram)\n","        ungkapan_score = self.ungkapan(bigram)\n","        if ungkapan_score==0:\n","            ungkapan_score = self.ungkapan(trigram)\n","        if ungkapan_score!=0:\n","            self.score = ungkapan_score\n","            self.prev_score = 0\n","            self.pre_max_pos[i-1] = 1\n","            self.pre_max_neg[i-1] = -1\n","            self.max_pos = self.pre_max_pos[i-2] #if len(self.pre_max_pos)>1 else 1\n","            self.max_neg = self.pre_max_neg[i-2] #if len(self.pre_max_neg)>1 else -1\n","            self.sentence_score[i-1] = re.sub(r'\\[\\d\\]','',self.sentence_score[i-1])\n","\n","    def cek_repeated_punctuation(self, next_term):\n","        if re.search(r'!{2,}',next_term) and self.score >=3: self.score+=1\n","        if re.search(r'!{2,}',next_term) and self.score <=-3: self.score-=1\n","\n","    def remove_extra_repeated_char(self, term):\n","        return re.sub(r'([A-Za-z])\\1{2,}',r'\\1',term)\n","    def plural_to_singular(self, term):\n","        return re.sub(r'([A-Za-z]+)\\-\\1', r'\\1',term)\n","    def classify(self):\n","        result = \"neutral\"\n","        try:\n","            if self.mean_conf:\n","                mean_p = np.mean(self.mean_pos)\n","                mean_n = np.mean(self.mean_neg)\n","                print (mean_p, mean_n)\n","                if mean_p > mean_n:\n","                    result = \"positive\"\n","                elif mean_p < mean_n and not self.is_tanya:\n","                    result = \"negative\"\n","                elif mean_p < mean_n and self.is_tanya:\n","                    result = \"neutral\"\n","            else:\n","                if abs(self.sentences_max_pos) > abs(self.sentences_max_neg):\n","                    result = \"positive\"\n","                elif abs(self.sentences_max_pos) < abs(self.sentences_max_neg):\n","                    result = \"negative\"\n","                elif abs(self.sentences_max_pos) == abs(self.sentences_max_neg):\n","                    result = \"neutral\"\n","        except:\n","            print (\"error \",self.sentences_max_pos, self.sentences_max_neg)\n","        return result\n","    def cek_neutral_term(self,terms,i):\n","        if terms[i-1] in self.neutral_term or terms[i+1] in self.neutral_term: self.score=1 \n","\n","    def main(self,sentence):\n","        self.neutral_term = ['jika','kalau']\n","        sentences = sentence.split('.')\n","        self.sentences_max_neg = -1\n","        self.sentences_max_pos = 1\n","        self.sentences_score = []\n","        self.sentences_text = []\n","        for sentence in sentences:\n","            self.max_neg = -1\n","            self.max_pos = 1\n","            self.mean_neg = [1]\n","            self.mean_pos = [1]\n","            self.sentence_score=[]\n","            terms = sentence.split()\n","            # terms = re.split(r'[\\s,.]',sentence)\n","            terms_length = len(terms)\n","            self.is_tanya = False\n","            self.sentence_text = ''\n","            # print self.max_pos, self.max_neg\n","            #SEMUA KALIMAT YANG MEMILIKI TANDA SERU MEMILIKI +ve minimal 2\n","            if self.exclamation_conf and re.search('!',sentence): self.max_pos = 2\n","            self.prev_score = 0\n","            self.pre_max_pos = []\n","            self.pre_max_neg = []\n","            for i,term in enumerate(terms):\n","                # repeated_term = ''\n","                is_extra_char = False\n","                plural = ''\n","                self.score = 0\n","                # if re.search(r'[A-Za-z\\-.]+',term):\n","                # print term\n","                if re.search(r'([A-Za-z])\\1{3,}',term):\n","                    is_extra_char = True\n","                    # repeated_term =term\n","                term = self.remove_extra_repeated_char(term)\n","                if re.search(r'([A-Za-z]+)\\-\\1',term):\n","                    plural = term\n","                    term = self.plural_to_singular(term)\n","                #GET SENTI SCORE#\n","                self.score = self.senti(term)\n","                # print \"senti score\",term, self.score\n","\n","                #NEGATION HANDLER#\n","                if self.negation_conf and self.score !=0 and i>0:self.cek_negationword(terms[i-1],terms[i-2])\n","                # print  \"negation score\",term, self.score\n","\n","                #BOOSTERWORD HANDLER#\n","                if self.booster_conf and self.score !=0 and i>0 and i<=(terms_length-1):self.cek_boosterword(terms[i-1])\n","                if self.booster_conf and self.score !=0 and i>=0 and i<(terms_length-1):self.cek_boosterword(terms[i+1])\n","                # print  \"booster score\",term, self.score\n","\n","                #IDIOM/UNGKAPAN HANDLER#\n","                if self.ungkapan_conf and i>0 and i<=(terms_length-1):self.cek_ungkapan([terms[i-1],term],[terms[i-2],terms[i-1],term],i)\n","                # if self.ungkapan_conf and i>=0 and i<(terms_length-1):self.cek_ungkapan([term,terms[i+1]])\n","                # print  \"idiom score\",term, self.score\n","\n","                #CONSECUTIVE SENTIMENT WORD#\n","                if self.consecutive_conf and i>0 and i<=(terms_length-1) and self.score !=0:self.cek_consecutive_term(terms[i-1])\n","                # print  \"consecutive score\",term, self.score\n","\n","                #+1 SENTI SCORE IF REPEATED CHAR ON POSITIVE/NEGATIVE +2 IF NEUTRAL TERM\n","                if self.repeated_conf and is_extra_char==True and self.score>0: self.score+=1\n","                if self.repeated_conf and is_extra_char==True and self.score<0: self.score-=1\n","                if self.repeated_conf and is_extra_char==True and self.score==0: self.score=2\n","                # print  \"repeat char score\", term, self.score\n","                if self.punctuation_conf and i>=0 and i<(terms_length-1): self.cek_repeated_punctuation(terms[i+1])\n","                # CEK APAKAH TERDAPAT KATA TANYA\n","                if self.question_conf and (term in self.tanya or re.search(r'\\?',term)):self.is_tanya = True\n","                # CEK neutral term \n","                if self.score!=0 and i>1 and i<(terms_length-2): self.cek_neutral_term(terms,i)\n","                # if self.score!=0 and i>0 and i<(terms_length-4): self.cek_neutral_term(terms,i)\n","                if self.emoticon_conf and self.score==0: self.score = self.emosikon(term)\n","\n","                self.prev_score = self.score\n","                if self.mean_conf and self.score>0: self.mean_pos.append(self.score)\t\n","                if self.mean_conf and self.score<0: self.mean_neg.append(abs(self.score))\n","                #GET MAX SCORE +ve/-ve\t\n","                self.max_pos= self.score if self.score > self.max_pos else self.max_pos\n","                self.max_neg= self.score if self.score < self.max_neg else self.max_neg\n","                #insert score info current term\n","                self.pre_max_pos.append(self.max_pos)\n","                self.pre_max_neg.append(self.max_neg)\n","                # print self.pre_max_pos, self.pre_max_neg\n","                if plural !='': term = plural\n","                self.sentence_text += ' {}'.format(term)\n","                if self.score != 0:term = \"{} [{}]\".format(term, self.score)\n","                self.sentence_score.append(term)\n","\n","            self.sentences_text.append(self.sentence_text)\n","            self.sentences_score.append(\" \".join(self.sentence_score))\n","            if self.is_tanya: \n","                self.max_neg = -1\n","            self.sentences_max_pos = self.max_pos if self.max_pos > self.sentences_max_pos else self.sentences_max_pos\n","            self.sentences_max_neg = self.max_neg if self.max_neg < self.sentences_max_neg else self.sentences_max_neg\n","            # print self.sentences_max_pos, self.sentences_max_neg\n","        sentence_result = self.classify()\n","        # print self.sentences_text\n","        return {\"classified_text\":\". \".join(self.sentences_score),\"tweet_text\":\". \".join(self.sentences_text),\"sentence_score\":self.sentences_score,\"max_positive\":self.sentences_max_pos,\"max_negative\":self.sentences_max_neg,\"kelas\":sentence_result}\n","\n","config = dict()\n","config[\"negation\"] = True\n","config[\"booster\"]  = True\n","config[\"ungkapan\"]  = True\n","config[\"consecutive\"]  = True\n","config[\"repeated\"]  = True\n","config[\"emoticon\"]  = True\n","config[\"question\"]  = True\n","config[\"exclamation\"]  = True\n","config[\"punctuation\"]  = True\n","senti = sentistrength(config)\n","\n","print (senti.main(\"hisyam korupsi ganteng\"))\n","\n","# text = input(\"Masukkan kalimat yang ingin dianalisis: \")\n","# print(senti.main(text))"],"metadata":{"id":"oSscG2NUUZnD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","\n","random_numbers = random.sample(range(502, 1002), 500)\n","print(random_numbers, end=\"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Jua1kNpZlNb","executionInfo":{"status":"ok","timestamp":1680097471946,"user_tz":-420,"elapsed":2,"user":{"displayName":"Hisyam Haryo","userId":"17266018885690485961"}},"outputId":"e77671a5-cee7-430c-cca8-7502c149bc4e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[606, 534, 941, 816, 829, 675, 774, 809, 869, 643, 938, 630, 951, 755, 734, 680, 834, 538, 971, 707, 579, 509, 835, 959, 633, 528, 546, 623, 860, 874, 729, 513, 870, 886, 687, 794, 555, 832, 992, 862, 980, 735, 536, 822, 916, 953, 681, 524, 969, 887, 742, 901, 723, 728, 994, 535, 746, 998, 686, 917, 548, 645, 702, 620, 803, 984, 611, 900, 650, 694, 812, 849, 576, 907, 639, 934, 927, 624, 857, 790, 845, 512, 590, 505, 556, 972, 537, 703, 888, 813, 830, 878, 714, 658, 955, 737, 926, 659, 679, 654, 905, 823, 843, 750, 665, 664, 807, 577, 882, 682, 943, 560, 551, 667, 741, 533, 603, 598, 731, 844, 773, 506, 991, 554, 804, 885, 909, 695, 597, 952, 697, 757, 831, 915, 622, 657, 818, 763, 767, 963, 632, 610, 820, 806, 815, 721, 616, 640, 596, 747, 942, 881, 511, 510, 613, 724, 502, 949, 768, 867, 760, 819, 572, 570, 612, 785, 935, 851, 772, 871, 846, 799, 891, 614, 766, 826, 521, 919, 520, 879, 732, 660, 974, 588, 744, 549, 911, 684, 642, 652, 699, 778, 990, 988, 840, 863, 789, 945, 691, 792, 564, 543, 668, 552, 921, 873, 762, 798, 508, 609, 711, 950, 973, 574, 758, 626, 661, 805, 997, 854, 582, 631, 653, 933, 975, 605, 692, 902, 655, 954, 996, 649, 578, 708, 765, 722, 743, 962, 565, 893, 503, 868, 693, 749, 912, 777, 547, 783, 751, 617, 808, 914, 677, 553, 910, 967, 526, 672, 797, 683, 781, 594, 706, 957, 539, 739, 514, 929, 904, 859, 810, 629, 903, 647, 756, 931, 696, 730, 719, 587, 531, 811, 542, 918, 615, 978, 932, 745, 898, 872, 787, 656, 833, 814, 628, 892, 600, 602, 580, 795, 848, 519, 621, 793, 637, 532, 726, 883, 856, 558, 800, 847, 710, 638, 671, 662, 566, 557, 670, 828, 571, 517, 674, 875, 999, 920, 827, 761, 889, 960, 688, 821, 604, 993, 663, 780, 865, 595, 764, 698, 976, 625, 897, 678, 836, 584, 627, 850, 718, 641, 982, 985, 899, 689, 983, 771, 738, 838, 986, 966, 946, 673, 544, 802, 593, 769, 666, 507, 720, 853, 1000, 563, 968, 924, 607, 585, 725, 839, 922, 608, 700, 788, 676, 748, 704, 923, 559, 562, 913, 715, 786, 529, 776, 925, 601, 589, 619, 618, 825, 895, 817, 779, 568, 583, 939, 775, 733, 592, 770, 648, 944, 928, 685, 837, 852, 516, 752, 877, 651, 636, 876, 824, 709, 504, 965, 522, 540, 796, 716, 523, 861, 545, 705, 569, 591, 644, 858, 964, 727, 669, 784, 987, 530, 573, 561, 995, 541, 1001, 930, 970, 753, 736, 518, 864, 884, 947, 890, 717, 989, 937, 646, 961, 567, 599, 958, 782, 759, 635, 894, 525, 906, 880, 801, 908, 866, 713, 586, 841, 981, 936, 791, 575, 701, 527, 634, 940, 956, 842, 754, 581, 740, 712, 896, 690, 977, 979, 948, 515, 855, 550]\n"]}]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNUSVNrcB1rZJwL6aM+AcVY"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}